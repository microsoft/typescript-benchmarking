# This pipeline handles all TypeScript benchmarking. Supported situations are:
#
# - Pushes to microsoft/TypeScript's main branch. This is triggered indirectly
#   via the ts-main-pipeline resource. In this situation, BuildReason will be
#   ResourceTrigger, and we will upload the results to the blob store.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/heads/main", "version": "<commit hash>" }
#       }
#     },
#     "templateParameters": {
#       "HISTORICAL_RUN": true
#     }
#   }
#   In this situation, we'll treat this as a backfill run on main and upload
#   the results to the blob store. This also works for refs/heads/release-*.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/pull/<number>/merge" }
#       }
#     }
#   }
#   In this situation, we'll treat this as a PR run, comparing the pull request
#   against its merge base with main. Its results will be published back to the
#   pull request as a comment. Note that this payload is not one that the UI
#   supports, but it's possible to manually trigger it via the API, in which
#   case the UI will properly render all details about the resources.

pr: none
trigger: none

resources:
  # Note that the repository identifiers are known to external callers; don't
  # change them without ensuring everything is updated.
  repositories:
    - repository: TypeScript
      type: github
      endpoint: Microsoft
      name: microsoft/TypeScript

  # https://stackoverflow.com/a/63276774
  pipelines:
    - pipeline: ts-main-pipeline
      source: 'TypeScript pipeline trigger'
      trigger:
        branches:
          include:
            - main
            - release-*

parameters:
  # TODO(jakebailey): allow custom preset
  - name: TSPERF_PRESET
    displayName: Preset
    # Note: keep this up to date with generateMatrix and https://github.com/microsoft/typescript-bot-test-triggerer
    values:
      - full
      - regular
      - tsc-only
      - bun
      - vscode
    default: full # Branch pushes use the defaults, so this is set to full.

  - name: HISTORICAL_RUN
    displayName: This is a historical run (only check this if you know what you're doing)
    type: boolean
    default: false

  # PR trigger params
  - name: REQUESTING_USER
    displayName: User to tag when the results are ready (PR runs only)
    type: string
    default: '«anyone?»'
  - name: SOURCE_ISSUE
    displayName: PR ID in github (PR runs only)
    type: number
    default: 0
  - name: STATUS_COMMENT
    displayName: typescript-bot comment ID indicating that the run started (PR runs only)
    type: number
    default: 0

  # Keep in sync with inventory.yml and generateMatrix.ts.
  - name: AGENTS
    displayName: Exhaustive list of agents
    type: string
    default: 'any,ts-perf1,ts-perf2,ts-perf3,ts-perf4'

variables:
  Codeql.Enabled: false
  skipComponentGovernanceDetection: true

  azureSubscription: 'TypeScript Public CI'
  KeyVaultName: 'jststeam-passwords'

  REF: $[ resources.repositories['TypeScript'].ref ]
  PRETTY_REF: $[ replace(replace(replace(replace(variables['REF'], '/merge', ''), 'refs/pull/', 'pr.'), 'refs/heads/', ''), '/', '_') ]
  IS_PR: $[ startsWith(variables['REF'], 'refs/pull/') ]
  # True if this run should demand a baseline machine.
  USE_BASELINE_MACHINE: $[ or(eq(variables['Build.Reason'], 'ResourceTrigger'), ${{ parameters.HISTORICAL_RUN }}) ]
  # Only upload if the provided ref is explicitly main/release-* and we've run on the baseline machine.
  # This means that main pushes will automatically upload, but we can still queue builds off of main with a specific commit in the API.
  SHOULD_UPLOAD: $[ and(or(eq(variables['REF'], 'refs/heads/main'), startsWith(variables['REF'], 'refs/heads/release-')), eq(variables['USE_BASELINE_MACHINE'], 'true')) ]
  # This is a hack to emulate a ternary operator.
  # https://github.com/microsoft/azure-pipelines-yaml/issues/256#issuecomment-824121862
  # Since this is annoying, here's a playground to help generate them: https://tsplay.dev/wO2DRW
  PRETTY_SUFFIX: $[ replace(replace(eq(variables['SHOULD_UPLOAD'], 'true'), True, replace(replace(eq(variables['Build.Reason'], 'ResourceTrigger'), True, '-push'), False, '-historical')), False, '') ]

name: $(PRETTY_REF)$(PRETTY_SUFFIX)-$(Date:yyyyMMdd).$(Rev:r)
# Hide the commit message from the run name; it'll just always say that the
# build came from the benchmarking repo's commit.
appendCommitMessageToRunName: false

jobs:
  - job: Setup
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts

    steps:
      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/generateMatrix.js --preset ${{ parameters.TSPERF_PRESET }}
        displayName: Generate matrix
        name: generateMatrix

      - checkout: TypeScript
        path: TypeScript
        fetchTags: false
        fetchDepth: 2 # For PRs, we want the merge base to compare against.
        clean: true
        retryCountOnTaskFailure: 3

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.js --outputDir $(ARTIFACTS_DIR)/pr
        displayName: Build PR TypeScript
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - bash: |
          set -eo pipefail
          git switch --detach HEAD^1
        displayName: Switch to merge base
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.js --baseline --outputDir $(ARTIFACTS_DIR)/baseline
        displayName: Build baseline TypeScript
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - publish: $(ARTIFACTS_DIR)
        artifact: BuiltTypeScript
        displayName: Publish built TypeScript

  - ${{ each agent in split(parameters.AGENTS, ',') }}:
      - job: Benchmark_${{ replace(agent, '-', '_') }}
        dependsOn: Setup
        condition: and(succeeded(), not(eq(dependencies.Setup.outputs['generateMatrix.MATRIX_${{ replace(agent, '-', '_') }}'], '{}')))
        pool:
          name: ts-perf-ddfun
          ${{ if not(eq(agent, 'any')) }}:
            demands: Agent.Name -equals ${{ agent }}
        workspace:
          clean: all # Always start with a clean slate.

        # https://stackoverflow.com/a/69345058
        strategy:
          matrix: $[ dependencies.Setup.outputs['generateMatrix.MATRIX_${{ replace(agent, '-', '_') }}'] ]

        variables:
          TSPERF_RUN_STARTUP: $[ or(eq(variables['TSPERF_JOB_KIND'], 'startup'), eq(variables['TSPERF_JOB_KIND'], 'all')) ]
          BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
          ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts

        steps:
          - task: AzureKeyVault@2
            inputs:
              azureSubscription: $(azureSubscription)
              KeyVaultName: $(KeyVaultName)
              SecretsFilter: 'tslab1-mseng-PAT'
            displayName: Get secrets

          - template: templates/setup.yml
          - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)
          - template: templates/cloneInternalRepo.yml # Sets $(TSPERF_INTERNAL_SCENARIO_CONFIG_DIR)

          - download: current
            artifact: BuiltTypeScript
            displayName: Download built TypeScript

          # This is provided by the agent.
          - bash: |
              set -eo pipefail
              sudo pyperf system tune
            displayName: Tune system

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js install-hosts
            displayName: Install hosts

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsc \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/pr \
                --save $(ARTIFACTS_DIR)/pr_$(TSPERF_JOB_NAME).tsc.benchmark
            displayName: Run PR tsc benchmark
            condition: and(succeeded(), eq(variables['IS_PR'], 'true'), eq(variables['TSPERF_TSC'], 'true'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsc \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).tsc.benchmark
            displayName: Run baseline tsc benchmark
            condition: and(succeeded(), eq(variables['TSPERF_TSC'], 'true'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsserver \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/pr \
                --save $(ARTIFACTS_DIR)/pr_$(TSPERF_JOB_NAME).tsserver.benchmark
            displayName: Run PR tsserver benchmark
            condition: and(succeeded(), eq(variables['IS_PR'], 'true'), eq(variables['TSPERF_TSSERVER'], 'true'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsserver \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).tsserver.benchmark
            displayName: Run baseline tsserver benchmark
            condition: and(succeeded(), eq(variables['TSPERF_TSSERVER'], 'true'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-startup \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/pr \
                --save $(ARTIFACTS_DIR)/pr_$(TSPERF_JOB_NAME).startup.benchmark
            displayName: Run PR startup benchmark
            condition: and(succeeded(), eq(variables['IS_PR'], 'true'), eq(variables['TSPERF_STARTUP'], 'true'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-startup \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).startup.benchmark
            displayName: Run baseline startup benchmark
            condition: and(succeeded(), eq(variables['TSPERF_STARTUP'], 'true'))

          - publish: $(ARTIFACTS_DIR)
            artifact: PartialBenchmark_$(TSPERF_JOB_NAME)
            displayName: Publish benchmarks

  - job: ProcessResults
    dependsOn:
      - Setup
      - ${{ each agent in split(parameters.AGENTS, ',') }}:
          - Benchmark_${{ replace(agent, '-', '_') }}
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      TSPERF_PROCESS_TSC: $[ dependencies.Setup.outputs['generateMatrix.TSPERF_PROCESS_TSC'] ]
      TSPERF_PROCESS_TSSERVER: $[ dependencies.Setup.outputs['generateMatrix.TSPERF_PROCESS_TSSERVER'] ]
      TSPERF_PROCESS_STARTUP: $[ dependencies.Setup.outputs['generateMatrix.TSPERF_PROCESS_STARTUP'] ]
      BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
      BENCHMARKS_DIR: $(Pipeline.Workspace)/benchmarks
      MERGED_DIR: $(Pipeline.Workspace)/merged

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'tslab1-mseng-PAT, tsperf-azure-storage-connection-string, typescript-bot-github-PAT-for-comments'
        displayName: Get secrets

      - template: templates/setup.yml

      - download: current
        patterns: '**/*.benchmark'
        displayName: Download benchmarks

      # Azure Pipelines haphazardly drops all artifacts into $(Pipeline.Workspace) with no
      # way to customize where they're placed. To deal with this, we download the benchmarks
      # first and move them somewhere else, before doing any other steps that would download
      # further artifacts.
      - bash: |
          set -eo pipefail
          shopt -s globstar
          mkdir -p $(BENCHMARKS_DIR)
          mv $(Pipeline.Workspace)/**/*.benchmark $(BENCHMARKS_DIR)
        displayName: Move artifacts

      # We only need this because ts-perf requires it even for loading existing benchmarks
      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)
      # TODO(jakebailey): remove this; see "suite" in ts-perf/packages/api/src/options.ts
      - template: templates/cloneInternalRepo.yml # Sets $(TSPERF_INTERNAL_SCENARIO_CONFIG_DIR)

      - bash: |
          set -eo pipefail
          mkdir -p $(MERGED_DIR)
        displayName: Create merged output directory

      # TODO(jakebailey): this is better than it used to be, but surely could still be less repetitive.
      - bash: |
          set -eo pipefail
          node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.tsc.benchmark $(BENCHMARKS_DIR)/baseline_*.tsc.benchmark

          if [[ $(REF) == refs/pull/* ]]; then
            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/pr.tsc.benchmark $(BENCHMARKS_DIR)/pr_*.tsc.benchmark

            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsc \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsc.benchmark \
              --load $(MERGED_DIR)/pr.tsc.benchmark

            echo "<h2>Compiler</h2>" >> $(Pipeline.Workspace)/comment.html
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsc \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsc.benchmark --baselineName baseline \
              --load $(MERGED_DIR)/pr.tsc.benchmark --benchmarkName pr \
              --format html-fragment \
              --quiet >> $(Pipeline.Workspace)/comment.html
          else
            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsc \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.tsc.benchmark
          fi
        displayName: Merge tsc benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_PROCESS_TSC'], 'true'))

      - bash: |
          set -eo pipefail
          node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.tsserver.benchmark $(BENCHMARKS_DIR)/baseline_*.tsserver.benchmark

          if [[ $(REF) == refs/pull/* ]]; then
            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/pr.tsserver.benchmark $(BENCHMARKS_DIR)/pr_*.tsserver.benchmark

            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsserver \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsserver.benchmark \
              --load $(MERGED_DIR)/pr.tsserver.benchmark

            echo "<h2>tsserver</h2>" >> $(Pipeline.Workspace)/comment.html
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsserver \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.tsserver.benchmark --baselineName baseline \
              --load $(MERGED_DIR)/pr.tsserver.benchmark --benchmarkName pr \
              --format html-fragment \
              --quiet >> $(Pipeline.Workspace)/comment.html
          else
            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsserver \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.tsserver.benchmark
          fi
        displayName: Merge tsserver benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_PROCESS_TSSERVER'], 'true'))

      - bash: |
          set -eo pipefail
          node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.startup.benchmark $(BENCHMARKS_DIR)/baseline_*.startup.benchmark

          if [[ $(REF) == refs/pull/* ]]; then
            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/pr.startup.benchmark $(BENCHMARKS_DIR)/pr_*.startup.benchmark

            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-startup \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.startup.benchmark \
              --load $(MERGED_DIR)/pr.startup.benchmark

            echo "<h2>Startup</h2>" >> $(Pipeline.Workspace)/comment.html
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-startup \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --baseline $(MERGED_DIR)/baseline.startup.benchmark --baselineName baseline \
              --load $(MERGED_DIR)/pr.startup.benchmark --benchmarkName pr \
              --format html-fragment \
              --quiet >> $(Pipeline.Workspace)/comment.html
          else
            # Print summary for logs
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-startup \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.startup.benchmark
          fi
        displayName: Merge startup benchmarks
        condition: and(succeeded(), eq(variables['TSPERF_PROCESS_STARTUP'], 'true'))

      - publish: $(MERGED_DIR)
        artifact: Benchmarks
        displayName: Publish benchmarks

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.js --fragment $(Pipeline.Workspace)/comment.html
        displayName: Publish PR comment
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        env:
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      # Sets $(TSPERF_BLOB_LATEST)
      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/checkLatestCommitForRef.js --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline
        displayName: Check if commit is latest for ref
        condition: and(succeeded(), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsc \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --load $(MERGED_DIR)/baseline.tsc.benchmark \
            --saveBlob linux
        displayName: Upload tsc benchmark to blob store
        condition: and(succeeded(), eq(variables['TSPERF_PROCESS_TSC'], 'true'), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-tsserver \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --load $(MERGED_DIR)/baseline.tsserver.benchmark \
            --saveBlob linux.server
        displayName: Upload tsserver benchmark to blob store
        condition: and(succeeded(), eq(variables['TSPERF_PROCESS_TSSERVER'], 'true'), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-startup \
            --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
            --load $(MERGED_DIR)/baseline.startup.benchmark \
            --saveBlob linux.startup
        displayName: Upload startup benchmark to blob store
        condition: and(succeeded(), eq(variables['TSPERF_PROCESS_STARTUP'], 'true'), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

  - job: OnFailedPRRun
    dependsOn:
      - Setup
      - ${{ each agent in split(parameters.AGENTS, ',') }}:
          - Benchmark_${{ replace(agent, '-', '_') }}
      - ProcessResults
    condition: and(failed(), eq(variables['IS_PR'], 'true'))
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'typescript-bot-github-PAT-for-comments'
        displayName: Get secrets

      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.js --failed
        displayName: Publish PR comment
        env:
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)
