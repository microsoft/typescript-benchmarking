# This pipeline handles all TypeScript benchmarking. Supported situations are:
#
# - Pushes to microsoft/TypeScript's main branch. This is triggered indirectly
#   via the ts-main-pipeline resource. In this situation, BuildReason will be
#   ResourceTrigger, and we will upload the results to the blob store.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/heads/main", "version": "<commit hash>" }
#       }
#     },
#     "templateParameters": {
#       "HISTORICAL_RUN": true
#     }
#   }
#   In this situation, we'll treat this as a backfill run on main and upload
#   the results to the blob store. This also works for refs/heads/release-*.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/pull/<number>/merge" }
#       }
#     }
#   }
#   In this situation, we'll treat this as a PR run, comparing the pull request
#   against its merge base with main. Its results will be published back to the
#   pull request as a comment. Note that this payload is not one that the UI
#   supports, but it's possible to manually trigger it via the API, in which
#   case the UI will properly render all details about the resources.

pr: none
trigger: none

resources:
  # Note that the repository identifiers are known to external callers; don't
  # change them without ensuring everything is updated.
  repositories:
    - repository: TypeScript
      type: github
      endpoint: Microsoft
      name: microsoft/TypeScript

  # https://stackoverflow.com/a/63276774
  pipelines:
    - pipeline: ts-main-pipeline
      source: 'TypeScript pipeline trigger'
      trigger:
        branches:
          include:
            - main
            - release-*

parameters:
  - name: TSPERF_INPUT
    displayName: Input (preset/args)
    default: ''

  - name: HISTORICAL_RUN
    displayName: This is a historical run (only check this if you know what you're doing)
    type: boolean
    default: false

  # PR trigger params
  - name: DISTINCT_ID
    displayName: Distinct ID for this run (PR runs only)
    type: string
    default: 'unknown'
  - name: REQUESTING_USER
    displayName: User to tag when the results are ready (PR runs only)
    type: string
    default: '«anyone?»'
  - name: SOURCE_ISSUE
    displayName: PR ID in github (PR runs only)
    type: number
    default: 0
  - name: STATUS_COMMENT
    displayName: typescript-bot comment ID indicating that the run started (PR runs only)
    type: number
    default: 0

  # Keep in sync with inventory.yml and setupPipeline.ts.
  - name: AGENTS
    displayName: Exhaustive list of agents
    type: string
    default: 'any,ts-perf1,ts-perf2,ts-perf3,ts-perf4,ts-perf5,ts-perf6,ts-perf7,ts-perf8,ts-perf9,ts-perf10,ts-perf11,ts-perf12'

variables:
  Codeql.Enabled: false
  skipComponentGovernanceDetection: true

  CI: true

  azureSubscription: 'TypeScript Public CI'
  KeyVaultName: 'jststeam-passwords'

  REF: $[ resources.repositories['TypeScript'].ref ]
  PRETTY_REF: $[ replace(replace(replace(replace(variables['REF'], '/merge', ''), 'refs/pull/', 'pr.'), 'refs/heads/', ''), '/', '_') ]
  IS_PR: $[ startsWith(variables['REF'], 'refs/pull/') ]
  # True if this run should demand a baseline machine.
  USE_BASELINE_MACHINE: $[ or(eq(variables['Build.Reason'], 'ResourceTrigger'), ${{ parameters.HISTORICAL_RUN }}) ]
  # Only upload if the provided ref is explicitly main/release-* and we've run on the baseline machine.
  # This means that main pushes will automatically upload, but we can still queue builds off of main with a specific commit in the API.
  SHOULD_UPLOAD: $[ and(or(eq(variables['REF'], 'refs/heads/main'), startsWith(variables['REF'], 'refs/heads/release-')), eq(variables['USE_BASELINE_MACHINE'], 'true')) ]
  # This is a hack to emulate a ternary operator.
  # https://github.com/microsoft/azure-pipelines-yaml/issues/256#issuecomment-824121862
  # Since this is annoying, here's a playground to help generate them: https://tsplay.dev/wO2DRW
  PRETTY_SUFFIX: $[ replace(replace(eq(variables['SHOULD_UPLOAD'], 'true'), True, replace(replace(eq(variables['Build.Reason'], 'ResourceTrigger'), True, '-push'), False, '-historical')), False, '') ]

name: $(PRETTY_REF)$(PRETTY_SUFFIX)-$(Date:yyyyMMdd).$(Rev:r)
# Hide the commit message from the run name; it'll just always say that the
# build came from the benchmarking repo's commit.
appendCommitMessageToRunName: false

jobs:
  - job: Setup
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts

    steps:
      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - checkout: TypeScript
        path: TypeScript
        fetchTags: false
        fetchDepth: 2 # For PRs, we want the merge base to compare against.
        clean: true
        retryCountOnTaskFailure: 3

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/setupPipeline.js
        displayName: Setup pipeline
        name: setupPipeline
        env:
          TSPERF_INPUT: ${{ parameters.TSPERF_INPUT }}

      - bash: |
          set -eo pipefail
          git switch --detach $(TSPERF_NEW_COMMIT)
        displayName: Switch to new commit
        condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.js --outputDir $(ARTIFACTS_DIR)/new
        displayName: Build new TypeScript
        condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))
        workingDirectory: $(Pipeline.Workspace)/TypeScript
        name: buildTypeScriptNew

      - bash: |
          set -eo pipefail
          git switch --detach $(TSPERF_BASELINE_COMMIT)
        displayName: Switch to baseline commit
        condition: and(succeeded(), not(eq(variables['TSPERF_BASELINE_COMMIT'], 'HEAD')))
        workingDirectory: $(Pipeline.Workspace)/TypeScript

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.js --baseline --outputDir $(ARTIFACTS_DIR)/baseline
        displayName: Build baseline TypeScript
        workingDirectory: $(Pipeline.Workspace)/TypeScript
        name: buildTypeScriptBaseline

      - publish: $(ARTIFACTS_DIR)
        artifact: BuiltTypeScript
        displayName: Publish built TypeScript

  - ${{ each agent in split(parameters.AGENTS, ',') }}:
      - job: Benchmark_${{ replace(agent, '-', '_') }}
        timeoutInMinutes: 360
        dependsOn: Setup
        condition: and(succeeded(), not(eq(dependencies.Setup.outputs['setupPipeline.MATRIX_${{ replace(agent, '-', '_') }}'], '{}')))
        pool:
          name: ts-perf-ddfun
          ${{ if not(eq(agent, 'any')) }}:
            demands: Agent.Name -equals ${{ agent }}
        workspace:
          clean: all # Always start with a clean slate.

        # https://stackoverflow.com/a/69345058
        strategy:
          matrix: $[ dependencies.Setup.outputs['setupPipeline.MATRIX_${{ replace(agent, '-', '_') }}'] ]

        variables:
          BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
          ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts
          TSPERF_IS_COMPARISON: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_IS_COMPARISON'] ]
          TYPESCRIPT_COMMIT_NEW: $[ dependencies.Setup.outputs['buildTypeScriptNew.TYPESCRIPT_COMMIT'] ]
          TYPESCRIPT_COMMIT_BASELINE: $[ dependencies.Setup.outputs['buildTypeScriptBaseline.TYPESCRIPT_COMMIT'] ]

        steps:
          - task: AzureKeyVault@2
            inputs:
              azureSubscription: $(azureSubscription)
              KeyVaultName: $(KeyVaultName)
              SecretsFilter: 'tslab1-mseng-PAT'
            displayName: Get secrets
            retryCountOnTaskFailure: 3

          - template: templates/setup.yml
          - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)
          - template: templates/cloneInternalRepo.yml # Sets $(TSPERF_INTERNAL_SCENARIO_CONFIG_DIR), $(TSPERF_INTERNAL_SUITE_DIR)
            parameters:
              condition: eq(variables['TSPERF_JOB_LOCATION'], 'internal')

          - download: current
            artifact: BuiltTypeScript
            displayName: Download built TypeScript

          # This is provided by the agent.
          - bash: |
              set -eo pipefail
              sudo pyperf system tune
            displayName: Tune system

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js install-hosts
            displayName: Install hosts

          - bash: |
              set -eo pipefail
              # Special case for self benchmark
              export TYPESCRIPT_COMMIT=$(TYPESCRIPT_COMMIT_NEW)
              SETUP=$(TSPERF_PUBLIC_SCENARIO_CONFIG_DIR)/$(TSPERF_JOB_SCENARIO)/setup.sh
              if test -f $SETUP; then
                echo "Running $SETUP"
                bash $SETUP
              fi
            displayName: Set up scenario for new benchmark
            condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'), eq(variables['TSPERF_JOB_LOCATION'], 'public'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-$(TSPERF_JOB_KIND) \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/new \
                --save $(ARTIFACTS_DIR)/new_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark
            displayName: Run new $(TSPERF_JOB_KIND) benchmark
            condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))

          - bash: |
              set -eo pipefail
              # Special case for self benchmark
              export TYPESCRIPT_COMMIT=$(TYPESCRIPT_COMMIT_BASELINE)
              SETUP=$(TSPERF_PUBLIC_SCENARIO_CONFIG_DIR)/$(TSPERF_JOB_SCENARIO)/setup.sh
              if test -f $SETUP; then
                echo "Running $SETUP"
                bash $SETUP
              fi
            displayName: Set up scenario for baseline benchmark
            condition: and(succeeded(), eq(variables['TSPERF_JOB_LOCATION'], 'public'))

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-$(TSPERF_JOB_KIND) \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark
            displayName: Run baseline $(TSPERF_JOB_KIND) benchmark

          - bash: |
              set -eo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-$(TSPERF_JOB_KIND) \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --baseline $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark \
                --load $(ARTIFACTS_DIR)/new_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark
            displayName: Compare new to baseline benchmark
            condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))

          - publish: $(ARTIFACTS_DIR)
            artifact: PartialBenchmark_$(TSPERF_JOB_NAME)
            displayName: Publish benchmarks

  - job: ProcessResults
    dependsOn:
      - Setup
      - ${{ each agent in split(parameters.AGENTS, ',') }}:
          - Benchmark_${{ replace(agent, '-', '_') }}
    # If any dependency was skipped (which will always be the case in the above matrix),
    # "succeeded()" will be false. Instead, only run when the build has not failed.
    # "not(failed())" is also true when the build has been canceled, so ignore that too.
    condition: and(not(failed()), not(canceled()))
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      TSPERF_IS_COMPARISON: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_IS_COMPARISON'] ]
      TSPERF_PROCESS_KINDS: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_PROCESS_KINDS'] ]
      TSPERF_PROCESS_LOCATIONS: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_PROCESS_LOCATIONS'] ]
      TSPERF_NEW_NAME: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_NEW_NAME'] ]
      TSPERF_BASELINE_NAME: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_BASELINE_NAME'] ]
      BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
      BENCHMARKS_DIR: $(Pipeline.Workspace)/benchmarks
      MERGED_DIR: $(Pipeline.Workspace)/merged

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'tslab1-mseng-PAT, tsperf-azure-storage-connection-string, typescript-bot-github-PAT-for-comments'
        displayName: Get secrets
        retryCountOnTaskFailure: 3

      - template: templates/setup.yml

      - download: current
        patterns: '**/*.benchmark'
        displayName: Download benchmarks

      # Azure Pipelines haphazardly drops all artifacts into $(Pipeline.Workspace) with no
      # way to customize where they're placed. To deal with this, we download the benchmarks
      # first and move them somewhere else, before doing any other steps that would download
      # further artifacts.
      - bash: |
          set -eo pipefail
          shopt -s globstar
          mkdir -p $(BENCHMARKS_DIR)
          mv $(Pipeline.Workspace)/**/*.benchmark $(BENCHMARKS_DIR)
        displayName: Move artifacts

      # We only need this because ts-perf requires it even for loading existing benchmarks
      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)
      # TODO(jakebailey): remove this; see "suite" in ts-perf/packages/api/src/options.ts
      - template: templates/cloneInternalRepo.yml # Sets $(TSPERF_INTERNAL_SCENARIO_CONFIG_DIR), $(TSPERF_INTERNAL_SUITE_DIR)

      - bash: |
          set -eo pipefail
          mkdir -p $(MERGED_DIR)
        displayName: Create merged output directory

      - bash: |
          set -eo pipefail

          for KIND in $(TSPERF_PROCESS_KINDS); do
            echo "Merging ${KIND} benchmarks"

            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.${KIND}.benchmark $(BENCHMARKS_DIR)/baseline_*.${KIND}.benchmark

            if [[ $(TSPERF_IS_COMPARISON) == "true" ]]; then
              node $(TSPERF_EXE) merge --output $(MERGED_DIR)/new.${KIND}.benchmark $(BENCHMARKS_DIR)/new_*.${KIND}.benchmark

              # Print summary for logs
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --baseline $(MERGED_DIR)/baseline.${KIND}.benchmark \
                --load $(MERGED_DIR)/new.${KIND}.benchmark

              echo "<h2>${KIND}</h2>" >> $(Pipeline.Workspace)/comment.html
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --baseline $(MERGED_DIR)/baseline.${KIND}.benchmark --baselineName $(TSPERF_BASELINE_NAME) \
                --load $(MERGED_DIR)/new.${KIND}.benchmark --benchmarkName $(TSPERF_NEW_NAME) \
                --format html-fragment \
                --quiet >> $(Pipeline.Workspace)/comment.html
            else
              # Print summary for logs
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --load $(MERGED_DIR)/baseline.${KIND}.benchmark
            fi
          done
        displayName: Merge benchmarks

      - publish: $(MERGED_DIR)
        artifact: Benchmarks
        displayName: Publish benchmark artifacts

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.js --fragment $(Pipeline.Workspace)/comment.html
        displayName: Publish PR comment
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        env:
          DISTINCT_ID: ${{ parameters.DISTINCT_ID }}
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      # Sets $(TSPERF_BLOB_LATEST)
      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/checkLatestCommitForRef.js --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline
        displayName: Check if commit is latest for ref
        condition: and(succeeded(), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      - bash: |
          set -eo pipefail
          for KIND in $(TSPERF_PROCESS_KINDS); do
            echo "Uploading ${KIND} benchmarks"
            case ${KIND} in 
              tsc)
                NAME="linux"
                ;;
              tsserver)
                NAME="linux.server"
                ;;
              *)
                NAME="linux.${KIND}"
                ;;
            esac
            node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
              --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
              --load $(MERGED_DIR)/baseline.${KIND}.benchmark \
              --saveBlob ${NAME}
          done
        displayName: Upload benchmarks to blob store
        condition: and(succeeded(), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          TSPERF_AZURE_STORAGE_CONNECTION_STRING: $(tsperf-azure-storage-connection-string)

  - job: OnFailedPRRun
    dependsOn:
      - Setup
      - ${{ each agent in split(parameters.AGENTS, ',') }}:
          - Benchmark_${{ replace(agent, '-', '_') }}
      - ProcessResults
    condition: and(failed(), eq(variables['IS_PR'], 'true'))
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'typescript-bot-github-PAT-for-comments'
        displayName: Get secrets
        retryCountOnTaskFailure: 3

      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - bash: |
          set -eo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.js --failed
        displayName: Publish PR comment
        env:
          DISTINCT_ID: ${{ parameters.DISTINCT_ID }}
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)
