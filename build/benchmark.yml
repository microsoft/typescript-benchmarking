# This pipeline handles all TypeScript benchmarking. Supported situations are:
#
# - Pushes to microsoft/TypeScript's main branch. This is triggered indirectly
#   via the ts-main-pipeline resource. In this situation, BuildReason will be
#   ResourceTrigger, and we will upload the results to the blob store.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/heads/main", "version": "<commit hash>" }
#       }
#     },
#     "templateParameters": {
#       "HISTORICAL_RUN": true
#     }
#   }
#   In this situation, we'll treat this as a backfill run on main and upload
#   the results to the blob store. This also works for refs/heads/release-*.
# - On manual trigger via the API, with a payload like:
#   {
#     "resources": {
#       "repositories": {
#         "TypeScript": { "refName": "refs/pull/<number>/merge" }
#       }
#     }
#   }
#   In this situation, we'll treat this as a PR run, comparing the pull request
#   against its merge base with main. Its results will be published back to the
#   pull request as a comment. Note that this payload is not one that the UI
#   supports, but it's possible to manually trigger it via the API, in which
#   case the UI will properly render all details about the resources.

pr: none
trigger: none

resources:
  # Note that the repository identifiers are known to external callers; don't
  # change them without ensuring everything is updated.
  repositories:
    - repository: TypeScript
      type: github
      endpoint: Microsoft
      name: microsoft/TypeScript

  # https://stackoverflow.com/a/63276774
  pipelines:
    - pipeline: ts-main-pipeline
      source: 'TypeScript pipeline trigger'
      trigger:
        branches:
          include:
            - main
            - release-*

parameters:
  - name: TSPERF_PRESET
    displayName: Input (preset/args)
    default: 'default'

  - name: HISTORICAL_RUN
    displayName: This is a historical run (only check this if you know what you're doing)
    type: boolean
    default: false

  # PR trigger params
  - name: DISTINCT_ID
    displayName: Distinct ID for this run (PR runs only)
    type: string
    default: 'unknown'
  - name: REQUESTING_USER
    displayName: User to tag when the results are ready (PR runs only)
    type: string
    default: '«anyone?»'
  - name: SOURCE_ISSUE
    displayName: PR ID in github (PR runs only)
    type: number
    default: 0
  - name: STATUS_COMMENT
    displayName: typescript-bot comment ID indicating that the run started (PR runs only)
    type: number
    default: 0

  # Keep in sync with inventory.yml and setupPipeline.ts.
  - name: AGENTS
    displayName: Exhaustive list of agents
    type: string
    default: 'any,ts-perf1,ts-perf2,ts-perf3,ts-perf4,ts-perf5,ts-perf6,ts-perf7,ts-perf8,ts-perf9,ts-perf10,ts-perf11,ts-perf12'

variables:
  Codeql.Enabled: false
  skipComponentGovernanceDetection: true

  CI: true

  azureSubscription: 'TypeScript Public CI'
  KeyVaultName: 'jststeam-passwords'

  REF: $[ resources.repositories['TypeScript'].ref ]
  PRETTY_REF: $[ replace(replace(replace(replace(variables['REF'], '/merge', ''), 'refs/pull/', 'pr.'), 'refs/heads/', ''), '/', '_') ]
  IS_PR: $[ startsWith(variables['REF'], 'refs/pull/') ]
  # True if this run should demand a baseline machine.
  USE_BASELINE_MACHINE: $[ or(eq(variables['Build.Reason'], 'ResourceTrigger'), ${{ parameters.HISTORICAL_RUN }}) ]
  # Only upload if the provided ref is explicitly main/release-* and we've run on the baseline machine.
  # This means that main pushes will automatically upload, but we can still queue builds off of main with a specific commit in the API.
  SHOULD_UPLOAD: $[ and(or(eq(variables['REF'], 'refs/heads/main'), startsWith(variables['REF'], 'refs/heads/release-')), eq(variables['USE_BASELINE_MACHINE'], 'true')) ]
  # This is a hack to emulate a ternary operator.
  # https://github.com/microsoft/azure-pipelines-yaml/issues/256#issuecomment-824121862
  # Since this is annoying, here's a playground to help generate them: https://tsplay.dev/wO2DRW
  PRETTY_SUFFIX: $[ replace(replace(eq(variables['SHOULD_UPLOAD'], 'true'), True, replace(replace(eq(variables['Build.Reason'], 'ResourceTrigger'), True, '-push'), False, '-historical')), False, '') ]

name: $(PRETTY_REF)$(PRETTY_SUFFIX)-$(Date:yyyyMMdd).$(Rev:r)
# Hide the commit message from the run name; it'll just always say that the
# build came from the benchmarking repo's commit.
appendCommitMessageToRunName: false

jobs:
  - job: Setup
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts
      TYPESCRIPT_DIR: $(Pipeline.Workspace)/TypeScript
      TSPERF_PRESET: ${{ parameters.TSPERF_PRESET }}

    steps:
      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - checkout: TypeScript
        path: TypeScript
        fetchTags: false
        fetchDepth: 2 # For PRs, we want the merge base to compare against.
        clean: true
        retryCountOnTaskFailure: 3

      # Ideally we'd just do this:
      #   git config remote.origin.fetch "+refs/heads/*:refs/remotes/origin/*"
      #   git config remote.origin.promisor true
      #   git config remote.origin.partialclonefilter blob:none
      #   git fetch --unshallow
      # But this is slower than recloning the entire repo.
      - bash: |
          set -exo pipefail
          OLD_REF=`git -C $(TYPESCRIPT_DIR) rev-parse HEAD`
          rm -rf $(TYPESCRIPT_DIR)
          git clone --filter=blob:none https://github.com/microsoft/TypeScript $(TYPESCRIPT_DIR)
          cd $(TYPESCRIPT_DIR)
          git fetch origin $(OLD_REF)
          git switch --detach FETCH_HEAD
          for i in `git branch -a | grep remote | grep -v HEAD | grep -v main`; do git branch --track ${i#remotes/origin/} $i; done
        displayName: Convert TypeScript clone to blobless clone
        # Only do this if one of the parameters looks like it's going to change the commit range.
        condition: and(succeeded(), contains(variables['TSPERF_PRESET'], 'commit'))

      - bash: |
          set -exo pipefail
          node $(BENCH_SCRIPTS)/setupPipeline.js
        displayName: Setup pipeline
        name: setupPipeline

      - bash: |
          set -exo pipefail
          git switch --detach $(TSPERF_NEW_COMMIT)
        displayName: Switch to new commit
        condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))
        workingDirectory: $(TYPESCRIPT_DIR)

      - bash: |
          set -exo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.js --outputDir $(ARTIFACTS_DIR)/new
        displayName: Build new TypeScript
        condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))
        workingDirectory: $(TYPESCRIPT_DIR)
        name: buildTypeScriptNew

      - bash: |
          set -exo pipefail
          git switch --detach $(TSPERF_BASELINE_COMMIT)
        displayName: Switch to baseline commit
        condition: and(succeeded(), not(eq(variables['TSPERF_BASELINE_COMMIT'], 'HEAD')))
        workingDirectory: $(TYPESCRIPT_DIR)

      - bash: |
          set -exo pipefail
          node $(BENCH_SCRIPTS)/buildTypeScript.js --baseline --outputDir $(ARTIFACTS_DIR)/baseline
        displayName: Build baseline TypeScript
        workingDirectory: $(TYPESCRIPT_DIR)
        name: buildTypeScriptBaseline

      - publish: $(ARTIFACTS_DIR)
        artifact: BuiltTypeScript
        displayName: Publish built TypeScript

  - ${{ each agent in split(parameters.AGENTS, ',') }}:
      - job: Benchmark_${{ replace(agent, '-', '_') }}
        timeoutInMinutes: 360
        dependsOn: Setup
        condition: and(succeeded(), not(eq(dependencies.Setup.outputs['setupPipeline.MATRIX_${{ replace(agent, '-', '_') }}'], '{}')))
        pool:
          name: ts-perf-ddfun
          ${{ if not(eq(agent, 'any')) }}:
            demands: Agent.Name -equals ${{ agent }}
        workspace:
          clean: all # Always start with a clean slate.

        # https://stackoverflow.com/a/69345058
        strategy:
          matrix: $[ dependencies.Setup.outputs['setupPipeline.MATRIX_${{ replace(agent, '-', '_') }}'] ]

        variables:
          BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
          ARTIFACTS_DIR: $(Pipeline.Workspace)/artifacts
          TSPERF_IS_COMPARISON: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_IS_COMPARISON'] ]
          TSPERF_PREDICTABLE: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_PREDICTABLE'] ]
          TYPESCRIPT_COMMIT_NEW: $[ dependencies.Setup.outputs['buildTypeScriptNew.TYPESCRIPT_COMMIT'] ]
          TYPESCRIPT_COMMIT_BASELINE: $[ dependencies.Setup.outputs['buildTypeScriptBaseline.TYPESCRIPT_COMMIT'] ]

        steps:
          - template: templates/setup.yml
          - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

          - download: current
            artifact: BuiltTypeScript
            displayName: Download built TypeScript

          # This is provided by the agent.
          - bash: |
              set -exo pipefail
              sudo pyperf system tune
            displayName: Tune system

          - bash: |
              set -exo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js install-hosts
            displayName: Install hosts
            retryCountOnTaskFailure: 3

          - bash: |
              set -exo pipefail
              # Special case for self benchmark
              export TYPESCRIPT_COMMIT=$(TYPESCRIPT_COMMIT_NEW)
              SETUP=$(TSPERF_SCENARIO_CONFIG_DIR)/$(TSPERF_JOB_SCENARIO)/setup.sh
              if test -f $SETUP; then
                echo "Running $SETUP"
                bash $SETUP
              fi
            displayName: Set up scenario for new benchmark
            condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))
            retryCountOnTaskFailure: 3

          - bash: |
              set -exo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-$(TSPERF_JOB_KIND) \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/new \
                --save $(ARTIFACTS_DIR)/new_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark
            displayName: Run new $(TSPERF_JOB_KIND) benchmark
            condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))

          - bash: |
              set -exo pipefail
              # Special case for self benchmark
              export TYPESCRIPT_COMMIT=$(TYPESCRIPT_COMMIT_BASELINE)
              SETUP=$(TSPERF_SCENARIO_CONFIG_DIR)/$(TSPERF_JOB_SCENARIO)/setup.sh
              if test -f $SETUP; then
                echo "Running $SETUP"
                bash $SETUP
              fi
            displayName: Set up scenario for baseline benchmark
            retryCountOnTaskFailure: 3

          - bash: |
              set -exo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-$(TSPERF_JOB_KIND) \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --save $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark
            displayName: Run baseline $(TSPERF_JOB_KIND) benchmark

          - bash: |
              set -exo pipefail
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-$(TSPERF_JOB_KIND) \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --baseline $(ARTIFACTS_DIR)/baseline_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark \
                --load $(ARTIFACTS_DIR)/new_$(TSPERF_JOB_NAME).$(TSPERF_JOB_KIND).benchmark
            displayName: Compare new to baseline benchmark
            condition: and(succeeded(), eq(variables['TSPERF_IS_COMPARISON'], 'true'))

          - publish: $(ARTIFACTS_DIR)
            artifact: PartialBenchmark_$(TSPERF_JOB_NAME)
            displayName: Publish benchmarks

  - job: ProcessResults
    dependsOn:
      - Setup
      - ${{ each agent in split(parameters.AGENTS, ',') }}:
          - Benchmark_${{ replace(agent, '-', '_') }}
    # If any dependency was skipped (which will always be the case in the above matrix),
    # "succeeded()" will be false. Instead, only run when the build has not failed.
    # "not(failed())" is also true when the build has been canceled, so ignore that too.
    condition: and(not(failed()), not(canceled()))
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    variables:
      TSPERF_IS_COMPARISON: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_IS_COMPARISON'] ]
      TSPERF_PROCESS_KINDS: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_PROCESS_KINDS'] ]
      TSPERF_NEW_NAME: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_NEW_NAME'] ]
      TSPERF_BASELINE_NAME: $[ dependencies.Setup.outputs['setupPipeline.TSPERF_BASELINE_NAME'] ]
      BUILT_TYPESCRIPT_DIR: $(Pipeline.Workspace)/BuiltTypeScript
      BENCHMARKS_DIR: $(Pipeline.Workspace)/benchmarks
      MERGED_DIR: $(Pipeline.Workspace)/merged

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'typescript-bot-github-PAT-for-comments'
        displayName: Get secrets
        retryCountOnTaskFailure: 3

      - template: templates/setup.yml

      - download: current
        patterns: '**/*.benchmark'
        displayName: Download benchmarks

      # Azure Pipelines haphazardly drops all artifacts into $(Pipeline.Workspace) with no
      # way to customize where they're placed. To deal with this, we download the benchmarks
      # first and move them somewhere else, before doing any other steps that would download
      # further artifacts.
      - bash: |
          set -exo pipefail
          shopt -s globstar
          mkdir -p $(BENCHMARKS_DIR)
          mv $(Pipeline.Workspace)/**/*.benchmark $(BENCHMARKS_DIR)
        displayName: Move artifacts

      # We only need this because ts-perf requires it even for loading existing benchmarks
      - download: current
        artifact: BuiltTypeScript
        displayName: Download built TypeScript

      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - bash: |
          set -exo pipefail
          mkdir -p $(MERGED_DIR)
        displayName: Create merged output directory

      - bash: |
          set -exo pipefail

          for KIND in $(TSPERF_PROCESS_KINDS); do
            echo "Merging ${KIND} benchmarks"

            node $(TSPERF_EXE) merge --output $(MERGED_DIR)/baseline.${KIND}.benchmark $(BENCHMARKS_DIR)/baseline_*.${KIND}.benchmark

            if [[ $(TSPERF_IS_COMPARISON) == "true" ]]; then
              node $(TSPERF_EXE) merge --output $(MERGED_DIR)/new.${KIND}.benchmark $(BENCHMARKS_DIR)/new_*.${KIND}.benchmark

              # Print summary for logs
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --baseline $(MERGED_DIR)/baseline.${KIND}.benchmark \
                --load $(MERGED_DIR)/new.${KIND}.benchmark

              echo "<h2>${KIND}</h2>" >> $(Pipeline.Workspace)/comment.html
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --baseline $(MERGED_DIR)/baseline.${KIND}.benchmark --baselineName $(TSPERF_BASELINE_NAME) \
                --load $(MERGED_DIR)/new.${KIND}.benchmark --benchmarkName $(TSPERF_NEW_NAME) \
                --format html-fragment \
                --quiet >> $(Pipeline.Workspace)/comment.html
            else
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --load $(MERGED_DIR)/baseline.${KIND}.benchmark
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --load $(MERGED_DIR)/baseline.${KIND}.benchmark \
                --format html-fragment \
                --quiet >> $(Pipeline.Workspace)/comment.html
            fi
          done
        displayName: Merge benchmarks

      - publish: $(MERGED_DIR)
        artifact: Benchmarks
        displayName: Publish benchmark artifacts

      - bash: |
          set -exo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.js --fragment $(Pipeline.Workspace)/comment.html
        displayName: Publish PR comment
        condition: and(succeeded(), eq(variables['IS_PR'], 'true'))
        env:
          DISTINCT_ID: ${{ parameters.DISTINCT_ID }}
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      # Sets $(TSPERF_BLOB_LATEST)
      - bash: |
          set -exo pipefail
          node $(BENCH_SCRIPTS)/checkLatestCommitForRef.js --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline
        displayName: Check if commit is latest for ref
        condition: and(succeeded(), eq(variables['SHOULD_UPLOAD'], 'true'))
        env:
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)

      - task: AzureCLI@2
        inputs:
          azureSubscription: TsPerfStorage
          scriptType: 'bash'
          scriptLocation: 'inlineScript'
          inlineScript: |
            set -exo pipefail
            for KIND in $(TSPERF_PROCESS_KINDS); do
              echo "Uploading ${KIND} benchmarks"
              case ${KIND} in
                tsc)
                  NAME="linux"
                  ;;
                tsserver)
                  NAME="linux.server"
                  ;;
                *)
                  NAME="linux.${KIND}"
                  ;;
              esac
              node $(BENCH_SCRIPTS)/runTsPerf.js benchmark-${KIND} \
                --builtDir $(BUILT_TYPESCRIPT_DIR)/baseline \
                --load $(MERGED_DIR)/baseline.${KIND}.benchmark \
                --saveBlob ${NAME}
            done
        displayName: Upload benchmarks to blob store
        condition: and(succeeded(), eq(variables['SHOULD_UPLOAD'], 'true'))

  - job: OnFailedPRRun
    dependsOn:
      - Setup
      - ${{ each agent in split(parameters.AGENTS, ',') }}:
          - Benchmark_${{ replace(agent, '-', '_') }}
      - ProcessResults
    condition: and(failed(), eq(variables['IS_PR'], 'true'))
    pool:
      vmImage: ubuntu-latest
    workspace:
      clean: all # Always start with a clean slate.

    steps:
      - task: AzureKeyVault@2
        inputs:
          azureSubscription: $(azureSubscription)
          KeyVaultName: $(KeyVaultName)
          SecretsFilter: 'typescript-bot-github-PAT-for-comments'
        displayName: Get secrets
        retryCountOnTaskFailure: 3

      - template: templates/setup.yml
      - template: templates/cloneAndBuildBenchmarkRepo.yml # Sets $(BENCH_SCRIPTS), $(TSPERF_EXE)

      - bash: |
          set -exo pipefail
          node $(BENCH_SCRIPTS)/postPerfResult.js --failed
        displayName: Publish PR comment
        env:
          DISTINCT_ID: ${{ parameters.DISTINCT_ID }}
          SOURCE_ISSUE: ${{ parameters.SOURCE_ISSUE }}
          REQUESTING_USER: ${{ parameters.REQUESTING_USER }}
          STATUS_COMMENT: ${{ parameters.STATUS_COMMENT }}
          GH_TOKEN: $(typescript-bot-github-PAT-for-comments)
